from functools import lru_cache
import re
from typing import Any, Dict, Literal, Optional, Tuple, Union

from bs4 import BeautifulSoup
import contractions
import nltk
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize, TreebankWordDetokenizer
from nltk import pos_tag
from sentence_transformers import SentenceTransformer
import spacy
import spacy.tokens
from textblob import TextBlob

import config
from config.embedding import FIXED_EMBEDDING_LENGTH, MODEL_SENTENCE_TRANSFORMER
from config.processing_text import INSTALL_MISSING_NLTK, MIN_CHUNK_LENGTH, STEPS
from indexing.components import Document
from logger.helper import timed_block
from logger.setup import LoggerManager


logger_manager = LoggerManager(
    module_name=__name__,
    project_name=config.LOGGING["project_name"],
    folder_path=config.LOGGING["folder_path"],
    max_size=config.LOGGING["max_size"],
    console_level=config.LOGGING["console_level"],
    file_level=config.LOGGING["file_level"],
)
logger_manager.setup_logger()

class TextProcessor:
    """A class for processing text data, including cleaning,
    tokenization, and embedding generation.

    Attributes
    ----------
    nlp : spacy.Language
        A spaCy language model for text processing.
    embedder_model : SentenceTransformer
        A model for generating sentence embeddings.
    embedding_size : int
        The size of the embeddings generated by the embedder model.
    FAST_FORWARD_PROCEDURES : Dict[str, Callable]
        A dictionary mapping fast-forward procedure names to their corresponding methods.
    IN_DEPTH_PROCEDURES : Dict[str, Callable]
        A dictionary mapping in-depth procedure names to their corresponding methods.
    fast_forward : Dict[str, bool]
        Configuration for fast-forward procedures.
    in_depth : Dict[str, bool]
        Configuration for in-depth procedures.

    Methods
    -------
    `clean_text(text: str, chunk_length: int)` -> str
        Clean the input text by converting it to lowercase and removing
        special characters.
    `remove_stopwords(text: str | spacy.tokens.Doc, lemmatization: bool)` -> str
        Remove stopwords and optionally lemmatize tokens.
    `lemmatize(text: str | spacy.tokens.Doc)` -> str
        Lemmatize the input text tokens.
    `remove_urls_emails(text: str)` -> str
        Remove URLs and email addresses from the input text.
    `remove_html_tags(text: str)` -> str
        Remove HTML tags from the input text.
    `expand_contractions(text: str)` -> str
        Expand contractions in the input text.
    `remove_digits(text: str)` -> str
        Remove all digit characters from the input text.
    `correct_spelling(text: str)` -> str
        Correct spelling errors in the input text.
    `replace_synonym(text: str)` -> str
        Replace each word in the input text with its most common synonym
        using WordNet.
    `smart_lowercase(text: str | spacy.tokens.Doc)` -> str
        Convert text to lowercase, preserving proper nouns.
    `filter_out_named_entities(text: str | spacy.tokens.Doc)` -> str
        Filter out named entities from the input text.
    `remove_extra_spaces(text: str)` -> str
        Remove extra spaces from the input text.
    `process_text(file: Document)` -> None
        Apply all pre-processing steps in the order specified in the
        configuration.
    `compute_embedding(file: Document)` -> None
        Compute the embeddings of the processed text chunks using the
        SentenceTransformer model.
    """

    special_char_pat = re.compile(r'[^a-z0-9 ]')
    extra_spaces_pat = re.compile(r'\s+')
    digits_pat = re.compile(r'\d+')
    url_pat = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+|www\.\S+', re.IGNORECASE)
    email_pat = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', re.IGNORECASE)

    def __init__(
            self,
            model_name: str = MODEL_SENTENCE_TRANSFORMER,
            fixed_length: Optional[int] = FIXED_EMBEDDING_LENGTH
        ) -> None:
        try:
            self.nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])
        except Exception as e:
            logger_manager.log_message(
                f"Error loading model `en_core_web_sm`: {e}",
                "ERROR"
            )
            raise

        try:
            self.embedder_model = SentenceTransformer(
                model_name,
                truncate_dim = fixed_length
            )
        except Exception as e:
            logger_manager.log_message(
                f"Error during instantiation of SentenceTransformer: {e}",
                "ERROR"
            )
            raise

        self.embedding_size = self.embedder_model.get_sentence_embedding_dimension() or 384

        self._init_procedures()

        logger_manager.log_message(
            f"Successfully initialised the {self.__class__.__name__} instance.",
            "DEBUG"
        )

    def clean_text(
            self,
            text: str,
            chunk_length: Optional[int] = MIN_CHUNK_LENGTH
        ) -> str:
        """Clean the input text by converting it to lowercase and removing
        special characters. If a minimum length has been specified
        (`chunk_length != None`) and the processed text is less than it,
        return an empty string.
        """
        if (isinstance(chunk_length, int) and chunk_length <= 0 or
            not isinstance(chunk_length, int) and chunk_length is not None):
            logger_manager.log_message(
                ("The minimum length of the chunks must be a positive integer. "
                f"Instead, {chunk_length} has been provided."),
                "ERROR"
            )
            raise ValueError("`chunk_length` must be a positive integer")

        text = self._validate_text(
            text, method_name="clean_text", expected_type=str) # type: ignore
        if not text:
            return ""

        # Identify any character in a string that is not a lowercase
        # letter, number or space (only spaces, no tabs or newline
        # characters), such as punctuation, special symbols or capital
        # letters.
        cleaned = self.special_char_pat.sub(' ', text.lower())

        # Replace multiple spaces with a single space & strip
        # leading/trailing spaces
        cleaned = self.remove_extra_spaces(cleaned)

        if chunk_length and len(cleaned) <= chunk_length:
            return ""
        return cleaned

    def remove_stopwords(
            self,
            text: Union[str, spacy.tokens.Doc],
            lemmatization: bool
        ) -> str:
        """Remove stopwords and, if required, lemmatise tokens."""
        text = self._validate_text(
            text,
            method_name="remove_stopwords",
            expected_type=(str, spacy.tokens.Doc)
        )
        if not text:
            return ""

        if isinstance(text, str):
            text = self.nlp(text)

        tokens = (token.lemma_ if lemmatization else token.text for token in text if not token.is_stop)
        return " ".join(tokens)

    @lru_cache(maxsize=None)
    def lemmatize(self, text: Union[str, spacy.tokens.Doc]) -> str:
        """Lemmatise the input text tokens."""
        text = self._validate_text(
            text,
            method_name="lemmatize",
            expected_type=(str, spacy.tokens.Doc)
        )
        if not text:
            return ""

        if isinstance(text, str):
            text = self.nlp(text)

        return " ".join(token.lemma_ for token in text)

    def remove_urls_emails(self, text: str) -> str:
        """Remove URLs and email addresses from the input text."""
        text = self._validate_text(
            text, method_name="remove_urls_emails", expected_type=str) # type: ignore
        if not text:
            return ""

        text = self.url_pat.sub('', text)  # URLs
        text = self.email_pat.sub('', text)  # Emails
        return text

    def remove_html_tags(self, text: str) -> str:
        """Remove HTML tags from the input text."""
        text = self._validate_text(
            text, method_name="remove_html_tags", expected_type=str) # type: ignore
        if not text:
            return ""

        try:
            return BeautifulSoup(text, "html.parser").get_text().strip()
        except Exception as e:
            logger_manager.log_message(
                f"Error parsing HTML: {e}",
                "ERROR",
                exc_info=True
            )
            return ""

    def expand_contractions(self, text: str) -> str:
        """Expand contractions in the input text."""
        text = self._validate_text(
            text, method_name="expand_contractions", expected_type=str) # type: ignore
        if not text:
            return ""

        try:
            return contractions.fix(text, slang=True) # type: ignore
        except Exception as e:
            logger_manager.log_message(
                f"Error expanding contractions: {e}",
                "ERROR",
                exc_info=True
            )
            return text

    def remove_digits(self, text: str) -> str:
        """Remove all digit characters from the input text."""
        text = self._validate_text(
            text, method_name="remove_digits", expected_type=str) # type: ignore
        if not text:
            return ""

        return self.digits_pat.sub('', text)
        # return ''.join(char for char in text if not char.isdigit())

    @lru_cache(maxsize=None)
    def correct_spelling(self, text: str) -> str:
        text = self._validate_text(
            text, method_name="correct_spelling", expected_type=str) # type: ignore
        if not text:
            return ""

        try:
            return str(TextBlob(text).correct())
        except Exception as e:
            logger_manager.log_message(
                f"Error during spelling correction: {e}",
                "ERROR",
                exc_info=True
            )
            return text

    def replace_synonym(self, text: str) -> str:
        """Replace each word in the input text with its most common synonym using WordNet."""
        text = self._validate_text(
            text, method_name="replace_synonym", expected_type=str) # type: ignore

        if not self.synonym_replacement_available:
            return text

        if not text:
            return ""

        @lru_cache(maxsize=None)
        def get_most_common_synonym(word: str, pos: str) -> str:
            """Retrieve the most common synonym for `word` considering its part of speech."""
            pos_map = {"N": wordnet.NOUN, "V": wordnet.VERB, "J": wordnet.ADJ, "R": wordnet.ADV}
            wn_pos = pos_map.get(pos[0].upper(), wordnet.NOUN)  # Default to NOUN

            synonyms = wordnet.synsets(word, wn_pos)
            if synonyms:
                # Handle multi-word synonyms
                synonym = synonyms[0].lemmas()[0].name().replace("_", " ")  # type: ignore
                return synonym.capitalize() if word.istitle() else synonym

            # Return original if no synonym found
            return word

        words = word_tokenize(text)
        words_with_pos = pos_tag(words)

        replaced_words = [get_most_common_synonym(word, pos) for word, pos in words_with_pos]

        # Detokenizer to correctly place punctuation
        return TreebankWordDetokenizer().detokenize(replaced_words)

    def smart_lowercase(self, text: Union[str, spacy.tokens.Doc]) -> str:
        """Convert text to lowercase, preserving proper nouns."""
        text = self._validate_text(
            text,
            method_name="smart_lowercase",
            expected_type=(str, spacy.tokens.Doc)
        )
        if not text:
            return ""

        if isinstance(text, str):
            text = self.nlp(text)

        return " ".join(
            (token.text if token.pos_ == "PROPN" else token.text.lower()
             for token in text)
        )

    def filter_out_named_entities(
            self,
            text: Union[str, spacy.tokens.Doc]
        ) -> str:
        """Filter out named entities from the input text."""
        text = self._validate_text(
            text,
            method_name="filter_out_named_entities",
            expected_type=(str, spacy.tokens.Doc)
        )
        if not text:
            return ""

        if isinstance(text, str):
            text = self.nlp(text)

        return " ".join((token.text for token in text if not token.ent_type_))

    def remove_extra_spaces(self, text: str) -> str:
        """Remove extra spaces from the input text."""
        text = self._validate_text(
            text, method_name="remove_extra_spaces", expected_type=str) # type: ignore
        if not text:
            return ""

        return self.extra_spaces_pat.sub(' ', text).strip()

    def process_text(self, file: Document) -> None:
        """Application of all pre-processing steps in the order they are specified in the configuration."""
        logger_manager.log_message(
            f"Text processing of the file `{file.metadata.title}` has begun.",
            "INFO"
        )

        with timed_block(f"Document processing took", logger_manager.get_logger()):
            for page in file.pages:
                step = int(len(page) * 0.2)  # Log about every 20% of completion
                for chunk_i, chunk in enumerate(page.chunks):
                    if bool(chunk.processed_content):
                        # Text processing for this chunk has already been done
                        logger_manager.log_message(
                            "Chunk already processed; it is skipped.", "WARNING")
                        continue

                    processed_text = chunk.raw_content
                    if any(self.fast_forward.values()):
                        processed_text = self._apply_fast_forward_procedures(chunk.raw_content)
                    elif any(self.in_depth.values()):
                        processed_text = self._apply_in_depth_procedures(chunk.raw_content)
                    
                    chunk.processed_content = processed_text

                    if step != 0 and (chunk_i + 1) % step == 0:
                        progress = (chunk_i + 1) / len(page) * 100
                        logger_manager.log_message(
                            f"{progress:.0f}% of page {page.number} text processing completed.",
                            "DEBUG"
                        )
                logger_manager.log_message(
                    f"Page {page.number} successfully processed.", "DEBUG")

        logger_manager.log_message(
            "Processing of current document content successfully completed.",
            "INFO"
        )

    def compute_embedding(self, file: Document) -> None:
        """Compute the embeddings of the processed text chunks using the
        `SentenceTransformer` model."""
        self.process_text(file)

        logger_manager.log_message(
            f"Embedding generation for the document `{file.metadata.title}`.",
            "INFO"
        )

        # Generate embeddings for the proocessed text using the
        # SentenceTransformer model
        for page in file.pages:
            with timed_block(f"Embedding generation for page {page.number} took", logger_manager.get_logger()):

                step = int(len(page) * 0.22)  # Log about every 22% of completion
                for chunk_i, chunk in enumerate(page.chunks):

                    if step != 0 and (chunk_i + 1) % step == 0:
                        progress = (chunk_i + 1) / len(page) * 100
                        logger_manager.log_message(
                            f"{progress:.0f}% of the embeddings generation completed",
                            "DEBUG"
                        )

                    if chunk.processed_content:
                        try:
                            chunk.embedding = self.embedder_model.encode(
                                chunk.processed_content, convert_to_tensor=True).tolist()
                        except Exception as e:
                            logger_manager.log_message(
                                f"Error in the generation of the embedding: {e}",
                                "WARNING"
                            )
                            chunk.embedding = [-0.0]*self.embedding_size

        logger_manager.log_message(
            "Embeddings for the current document were elaborated.", "INFO")

    def _validate_text(
            self,
            text: Union[str, spacy.tokens.Doc],
            method_name: str,
            expected_type: Union[type, Tuple[type, ...]]
        ) -> Union[str, spacy.tokens.Doc]:
        """Validate the type of the input text.

        Parameters
        ----------
        text : Union[str, spacy.tokens.Doc]
            The text to be validated.
        method_name : str
            The name of the method where the validation is performed.
        expected_type : Union[type, Tuple[type, ...]]
            The expected type(s) for the text.

        Returns
        -------
        Union[str, spacy.tokens.Doc]
            The validated text if it matches the expected type.

        Raises
        ------
        TypeError
            If the text is not of the expected type.
        """
        try:
            expected_types = (
                "' or '".join((t.__name__ for t in expected_type))
                if isinstance(expected_type, tuple) else expected_type.__name__
            )
            if not isinstance(text, expected_type):
                logger_manager.log_message(
                    (f"Invalid input type for `text` in `{method_name}` method. "
                    f"Expected '{expected_types}', got '{type(text).__name__}'."),
                    "ERROR"
                )
                raise TypeError(f"Input must be of type '{expected_types}'")
            return text
        except Exception as e:
            logger_manager.log_message(
                f"Exception occurred during text validation in `{method_name}`: {e}",
                "ERROR",
                exc_info=True
            )
            raise

    def _init_procedures(self, force_installation: bool = INSTALL_MISSING_NLTK) -> None:
        """Initialize text processing procedures for fast-forward and in-depth modes.

        This method sets up dictionaries mapping procedure names to their
        corresponding methods for both fast-forward and in-depth text
        processing. It retrieves the configuration for each mode from the
        `STEPS` dictionary and validates the procedures. If both modes are
        activated, a warning is logged, and the fast-forward procedure is
        applied by default.
        """
        self.FAST_FORWARD_PROCEDURES = {
            "cleanUpText": self.clean_text,
            "stopWordsRemoval": lambda text: self.remove_stopwords(
                text, lemmatization=False),
            "wordLemmatization": self.lemmatize
        }
        self.IN_DEPTH_PROCEDURES = {
            "urlEmailRemoval": self.remove_urls_emails,
            "htmlTagRemoval": self.remove_html_tags,
            "contractionExpansion": self.expand_contractions,
            "digitsRemoval": self.remove_digits,
            "handlingSpellingErrors": self.correct_spelling,
            "synonymReplacement": self.replace_synonym,
            "smartLowercasing": self.smart_lowercase,
            "entityMasking": self.filter_out_named_entities,
            "stopWordsRemoval": lambda text: self.remove_stopwords(
                text, lemmatization=False),
            "wordLemmatization": self.lemmatize,
            "extraWhitespacesRemoval": self.remove_extra_spaces,
            "cleanUpText": self.clean_text
        }
        self.fast_forward = STEPS.get("fastForward", {})
        self.in_depth = STEPS.get("inDepth", {})

        self._validate_procedures(
            self.fast_forward, self.FAST_FORWARD_PROCEDURES, "fastForward")
        self._validate_procedures(
            self.in_depth, self.IN_DEPTH_PROCEDURES, "inDepth")

        self.ff_is_active = any(self.fast_forward.values())
        self.id_is_active = any(self.in_depth.values())

        if not self.ff_is_active and  not self.id_is_active:
            logger_manager.log_message(
                "No procedure was activated neither `fastForward` nor `inDepth`.",
                "WARNING"
            )
        else:
            if self.ff_is_active and self.id_is_active:
                logger_manager.log_message(
                    ("Both the `fastForward` and `inDepth` procedures have been "
                    "activated, but only one of them can be applied. The "
                    "`fastForward` procedure is applied."),
                    "WARNING"
                )


            logger_manager.log_message(
                "Text is processed through the `{}` procedure(s): '{}'.".format(
                    "fastForward" if self.ff_is_active else "inDepth",
                    "', '".join(list(self.fast_forward.keys()))
                    if self.ff_is_active else "', '".join(list(self.in_depth.keys()))
                ),
                "DEBUG"
            )

        self._check_nltk_dependencies(force_installation)

    def _check_nltk_dependencies(self, force_installation: bool) -> None:
        # Check if wordnet is available
        self.synonym_replacement_available = True
        try:
            wordnet.ensure_loaded()
        except LookupError:
            if force_installation:
                logger_manager.log_message(
                    "The Wordnet corpus is not available, so it is downloaded.",
                    "WARNING"
                )
                nltk.download('wordnet')
            else:
                logger_manager.log_message(
                    "Wordnet corpus is not available. Please download it using nltk.download('wordnet').",
                    "WARNING"
                )
                self.synonym_replacement_available = False

        # Check if 'punkt' is available
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            if force_installation:
                logger_manager.log_message(
                    "The Punkt data is not available, so it is downloaded.",
                    "WARNING"
                )
                nltk.download('punkt')
            else:
                logger_manager.log_message(
                    "Punkt data is not available. Please download it using nltk.download('punkt').",
                    "WARNING"
                )
                self.synonym_replacement_available = False

        # Check if 'punkt' is available
        try:
            nltk.data.find('taggers/averaged_perceptron_tagger_eng')
        except LookupError:
            if force_installation:
                logger_manager.log_message(
                    "Tagger data is not available, so it is downloaded.",
                    "WARNING"
                )
                nltk.download('averaged_perceptron_tagger_eng')
            else:
                logger_manager.log_message(
                    "Tagger data is not available. Please download it using nltk.download('averaged_perceptron_tagger_eng').",
                    "WARNING"
                )
                self.synonym_replacement_available = False

    def _validate_procedures(
            self,
            procedures: Dict[str, Any],
            valid_procedures: Dict[str, Any],
            procedure_type: Literal["fastForward", "inDepth"]
        ) -> None:
        """Validate the provided procedures against the valid procedures.

        Parameters
        ----------
        procedures : Dict[str, Any]
            The procedures to be validated.
        valid_procedures : Dict[str, Any]
            The dictionary of valid procedures.
        procedure_type : Literal["fastForward", "inDepth"]
            The type of procedure being validated.

        Raises
        ------
        KeyError
            If any procedure in `procedures` is not found in
            `valid_procedures`.
        """
        err = set(procedures) - set(valid_procedures)
        if err:
            logger_manager.log_message(
                (f"`{procedure_type}` procedures can only be '{"', '".join(valid_procedures)}'. "
                 f"Instead, {', '.join(err)} were also provided."),
                "ERROR"
            )
            raise KeyError(f"Invalid procedure(s): {', '.join(err)}")

    def _apply_fast_forward_procedures(self, text: str) -> str:
        for name, is_active in self.fast_forward.items():
            if is_active and name in self.FAST_FORWARD_PROCEDURES:
                text = self.FAST_FORWARD_PROCEDURES[name](text)
        return text

    def _apply_in_depth_procedures(self, text: str) -> str:
        for name, is_active in self.in_depth.items():
            if is_active and name in self.IN_DEPTH_PROCEDURES:
                text = self.IN_DEPTH_PROCEDURES[name](text)
        return text